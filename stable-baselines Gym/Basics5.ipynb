{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gymnasium as gym\n",
    "import stable_baselines3 as sb\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renders the model in the enviroment to see it learning progress\n",
    "def show_progress(model, time_steps = 1000, deterministic = True):\n",
    "    env = model.get_env()\n",
    "    obs = env.reset()\n",
    "    for i in range(time_steps):\n",
    "        action, _states = model.predict(obs, deterministic = deterministic)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        env.render(\"human\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym Env\n",
    "Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "\n",
    "class GoLeftEnv(gym.Env):\n",
    "  \"\"\"\n",
    "  Custom Environment that follows gym interface.\n",
    "  This is a simple env where the agent must learn to go always left. \n",
    "  \"\"\"\n",
    "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "  metadata = {'render.modes': ['console']}\n",
    "  # Define constants for clearer code\n",
    "  LEFT = 0\n",
    "  RIGHT = 1\n",
    "\n",
    "  def __init__(self, grid_size=10):\n",
    "    super(GoLeftEnv, self).__init__()\n",
    "\n",
    "    # Size of the 1D-grid\n",
    "    self.grid_size = grid_size\n",
    "    # Initialize the agent at the right of the grid\n",
    "    self.agent_pos = grid_size - 1\n",
    "\n",
    "    # Define action and observation space\n",
    "    # They must be gym.spaces objects\n",
    "    # Example when using discrete actions, we have two: left and right\n",
    "    n_actions = 2\n",
    "    self.action_space = spaces.Discrete(n_actions)\n",
    "    # The observation will be the coordinate of the agent\n",
    "    # this can be described both by Discrete and Box space\n",
    "    self.observation_space = spaces.Box(low=0, high=self.grid_size,\n",
    "                                        shape=(1,), dtype=np.float32)\n",
    "\n",
    "  def reset(self, seed = 0):\n",
    "    \"\"\"\n",
    "    Important: the observation must be a numpy array\n",
    "    :return: (np.array) \n",
    "    \"\"\"\n",
    "    # Initialize the agent at the right of the grid\n",
    "    self.agent_pos = self.grid_size - 1\n",
    "    # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "    return np.array([self.agent_pos]).astype(np.float32), {}\n",
    "\n",
    "  def step(self, action):\n",
    "    if action == self.LEFT:\n",
    "      self.agent_pos -= 1\n",
    "    elif action == self.RIGHT:\n",
    "      self.agent_pos += 1\n",
    "    else:\n",
    "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
    "\n",
    "    # Account for the boundaries of the grid\n",
    "    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
    "\n",
    "    # Are we at the left of the grid?\n",
    "    terminated = bool(self.agent_pos == 0)\n",
    "    truncated = False\n",
    "    \n",
    "    # Null reward everywhere except when reaching the goal (left of the grid)\n",
    "    reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "    # Optionally we can pass additional info, we are not using that for now\n",
    "    info = {}\n",
    "\n",
    "    return np.array([self.agent_pos]).astype(np.float32),\\\n",
    "            reward, terminated, truncated, info\n",
    "\n",
    "  def render(self, mode='console'):\n",
    "    if mode != 'console':\n",
    "      raise NotImplementedError()\n",
    "    # agent is represented as a cross, rest as a dot\n",
    "    print(\".\" * self.agent_pos, end=\"\")\n",
    "    print(\"x\", end=\"\")\n",
    "    print(\".\" * (self.grid_size - self.agent_pos))\n",
    "\n",
    "  def close(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Env check and test with a coded agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GoLeftEnv()\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)\n",
    "\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "GO_LEFT = 0\n",
    "# Hardcoded best agent: always go left!\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "  print(\"Step {}\".format(step + 1))\n",
    "  obs, reward, terminated, truncated, info = env.step(GO_LEFT)\n",
    "  print('obs=', obs, 'reward=', reward, 'done=', terminated)\n",
    "  env.render()\n",
    "  if terminated:\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora con un modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 761      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.688   |\n",
      "|    explained_variance | -121     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.00109  |\n",
      "|    value_loss         | 5.3e-06  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 499      |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 820      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.674   |\n",
      "|    explained_variance | -18.5    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.00194  |\n",
      "|    value_loss         | 7.04e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 34.7     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 824      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.179   |\n",
      "|    explained_variance | -3.3     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.00128 |\n",
      "|    value_loss         | 0.00387  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 20.6     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 817      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0733  |\n",
      "|    explained_variance | 0.775    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.000462 |\n",
      "|    value_loss         | 0.00113  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.4      |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 826      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.141   |\n",
      "|    explained_variance | -1.74    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.0103   |\n",
      "|    value_loss         | 0.000817 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 9.34      |\n",
      "|    ep_rew_mean        | 1         |\n",
      "| time/                 |           |\n",
      "|    fps                | 838       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0259   |\n",
      "|    explained_variance | 0.69      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -3.82e-05 |\n",
      "|    value_loss         | 0.000168  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.16     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 845      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0356  |\n",
      "|    explained_variance | 0.736    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.00031  |\n",
      "|    value_loss         | 0.00129  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 9.14      |\n",
      "|    ep_rew_mean        | 1         |\n",
      "| time/                 |           |\n",
      "|    fps                | 853       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0115   |\n",
      "|    explained_variance | -0.974    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -4.98e-05 |\n",
      "|    value_loss         | 0.00136   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.06     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 857      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00375 |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 3.52e-06 |\n",
      "|    value_loss         | 7.07e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9        |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 846      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0106  |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 3.43e-05 |\n",
      "|    value_loss         | 0.000381 |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = GoLeftEnv(grid_size=10)\n",
    "# wrap it\n",
    "env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "model = sb.A2C('MlpPolicy', env, verbose=1).learn(5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pendulum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
