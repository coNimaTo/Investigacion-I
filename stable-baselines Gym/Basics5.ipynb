{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gymnasium as gym\n",
    "import stable_baselines3 as sb\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renders the model in the enviroment to see it learning progress\n",
    "def show_progress(model, time_steps = 1000, deterministic = True):\n",
    "    env = model.get_env()\n",
    "    obs = env.reset()\n",
    "    for i in range(time_steps):\n",
    "        action, _states = model.predict(obs, deterministic = deterministic)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        env.render(\"human\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym Env\n",
    "Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "\n",
    "class GoLeftEnv(gym.Env):\n",
    "  \"\"\"\n",
    "  Custom Environment that follows gym interface.\n",
    "  This is a simple env where the agent must learn to go always left. \n",
    "  \"\"\"\n",
    "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "  metadata = {'render.modes': ['console']}\n",
    "  # Define constants for clearer code\n",
    "  LEFT = 0\n",
    "  RIGHT = 1\n",
    "\n",
    "  def __init__(self, grid_size=10):\n",
    "    super(GoLeftEnv, self).__init__()\n",
    "\n",
    "    # Size of the 1D-grid\n",
    "    self.grid_size = grid_size\n",
    "    # Initialize the agent at the right of the grid\n",
    "    self.agent_pos = grid_size - 1\n",
    "\n",
    "    # Define action and observation space\n",
    "    # They must be gym.spaces objects\n",
    "    # Example when using discrete actions, we have two: left and right\n",
    "    n_actions = 2\n",
    "    self.action_space = spaces.Discrete(n_actions)\n",
    "    # The observation will be the coordinate of the agent\n",
    "    # this can be described both by Discrete and Box space\n",
    "    self.observation_space = spaces.Box(low=0, high=self.grid_size,\n",
    "                                        shape=(1,), dtype=np.float32)\n",
    "\n",
    "  def reset(self, seed = 0):\n",
    "    \"\"\"\n",
    "    Important: the observation must be a numpy array\n",
    "    :return: (np.array) \n",
    "    \"\"\"\n",
    "    # Initialize the agent at the right of the grid\n",
    "    self.agent_pos = self.grid_size - 1\n",
    "    # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "    return np.array([self.agent_pos]).astype(np.float32), {}\n",
    "\n",
    "  def step(self, action):\n",
    "    if action == self.LEFT:\n",
    "      self.agent_pos -= 1\n",
    "    elif action == self.RIGHT:\n",
    "      self.agent_pos += 1\n",
    "    else:\n",
    "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
    "\n",
    "    # Account for the boundaries of the grid\n",
    "    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
    "\n",
    "    # Are we at the left of the grid?\n",
    "    terminated = bool(self.agent_pos == 0)\n",
    "    truncated = False\n",
    "    \n",
    "    # Null reward everywhere except when reaching the goal (left of the grid)\n",
    "    reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "    # Optionally we can pass additional info, we are not using that for now\n",
    "    info = {}\n",
    "\n",
    "    return np.array([self.agent_pos]).astype(np.float32),\\\n",
    "            reward, terminated, truncated, info\n",
    "\n",
    "  def render(self, mode='console'):\n",
    "    if mode != 'console':\n",
    "      raise NotImplementedError()\n",
    "    # agent is represented as a cross, rest as a dot\n",
    "    print(\".\" * self.agent_pos, end=\"\")\n",
    "    print(\"x\", end=\"\")\n",
    "    print(\".\" * (self.grid_size - self.agent_pos))\n",
    "\n",
    "  def close(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Env check and test with a coded agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GoLeftEnv()\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)\n",
    "\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "GO_LEFT = 0\n",
    "# Hardcoded best agent: always go left!\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "  print(\"Step {}\".format(step + 1))\n",
    "  obs, reward, terminated, truncated, info = env.step(GO_LEFT)\n",
    "  print('obs=', obs, 'reward=', reward, 'done=', terminated)\n",
    "  env.render()\n",
    "  if terminated:\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora con un modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GoLeftEnv(grid_size=10)\n",
    "# wrap it\n",
    "env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "model = sb.A2C('MlpPolicy', env, verbose=1).learn(5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pendulum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
