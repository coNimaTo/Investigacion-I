{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gymnasium as gym\n",
    "import stable_baselines3 as sb\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renders the model in the enviroment to see it learning progress\n",
    "def show_progress(model, time_steps = 1000, deterministic = True):\n",
    "    env = model.get_env()\n",
    "    obs = env.reset()\n",
    "    for i in range(time_steps):\n",
    "        action, _states = model.predict(obs, deterministic = deterministic)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        env.render(\"human\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym Env\n",
    "Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "\n",
    "class GoLeftEnv(gym.Env):\n",
    "  \"\"\"\n",
    "  Custom Environment that follows gym interface.\n",
    "  This is a simple env where the agent must learn to go always left. \n",
    "  \"\"\"\n",
    "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "  metadata = {'render.modes': ['console']}\n",
    "  # Define constants for clearer code\n",
    "  LEFT = 0\n",
    "  RIGHT = 1\n",
    "\n",
    "  def __init__(self, grid_size=10):\n",
    "    super(GoLeftEnv, self).__init__()\n",
    "\n",
    "    # Size of the 1D-grid\n",
    "    self.grid_size = grid_size\n",
    "    # Initialize the agent at the right of the grid\n",
    "    self.agent_pos = grid_size - 1\n",
    "\n",
    "    # Define action and observation space\n",
    "    # They must be gym.spaces objects\n",
    "    # Example when using discrete actions, we have two: left and right\n",
    "    n_actions = 2\n",
    "    self.action_space = spaces.Discrete(n_actions)\n",
    "    # The observation will be the coordinate of the agent\n",
    "    # this can be described both by Discrete and Box space\n",
    "    self.observation_space = spaces.Box(low=0, high=self.grid_size,\n",
    "                                        shape=(1,), dtype=np.float32)\n",
    "\n",
    "  def reset(self, seed = 0):\n",
    "    \"\"\"\n",
    "    Important: the observation must be a numpy array\n",
    "    :return: (np.array) \n",
    "    \"\"\"\n",
    "    # Initialize the agent at the right of the grid\n",
    "    self.agent_pos = self.grid_size - 1\n",
    "    # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "    return np.array([self.agent_pos]).astype(np.float32), {}\n",
    "\n",
    "  def step(self, action):\n",
    "    if action == self.LEFT:\n",
    "      self.agent_pos -= 1\n",
    "    elif action == self.RIGHT:\n",
    "      self.agent_pos += 1\n",
    "    else:\n",
    "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
    "\n",
    "    # Account for the boundaries of the grid\n",
    "    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
    "\n",
    "    # Are we at the left of the grid?\n",
    "    terminated = bool(self.agent_pos == 0)\n",
    "    truncated = False\n",
    "    \n",
    "    # Null reward everywhere except when reaching the goal (left of the grid)\n",
    "    reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "    # Optionally we can pass additional info, we are not using that for now\n",
    "    info = {}\n",
    "\n",
    "    return np.array([self.agent_pos]).astype(np.float32),\\\n",
    "            reward, terminated, truncated, info\n",
    "\n",
    "  def render(self, mode='console'):\n",
    "    if mode != 'console':\n",
    "      raise NotImplementedError()\n",
    "    # agent is represented as a cross, rest as a dot\n",
    "    print(\".\" * self.agent_pos, end=\"\")\n",
    "    print(\"x\", end=\"\")\n",
    "    print(\".\" * (self.grid_size - self.agent_pos))\n",
    "\n",
    "  def close(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Env check and test with a coded agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........x.\n",
      "Box(0.0, 10.0, (1,), float32)\n",
      "Discrete(2)\n",
      "Step 1\n",
      "obs= [8.] reward= 0 done= False\n",
      "........x..\n",
      "Step 2\n",
      "obs= [7.] reward= 0 done= False\n",
      ".......x...\n",
      "Step 3\n",
      "obs= [6.] reward= 0 done= False\n",
      "......x....\n",
      "Step 4\n",
      "obs= [5.] reward= 0 done= False\n",
      ".....x.....\n",
      "Step 5\n",
      "obs= [4.] reward= 0 done= False\n",
      "....x......\n",
      "Step 6\n",
      "obs= [3.] reward= 0 done= False\n",
      "...x.......\n",
      "Step 7\n",
      "obs= [2.] reward= 0 done= False\n",
      "..x........\n",
      "Step 8\n",
      "obs= [1.] reward= 0 done= False\n",
      ".x.........\n",
      "Step 9\n",
      "obs= [0.] reward= 1 done= True\n",
      "x..........\n",
      "Goal reached! reward= 1\n"
     ]
    }
   ],
   "source": [
    "env = GoLeftEnv()\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)\n",
    "\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "GO_LEFT = 0\n",
    "# Hardcoded best agent: always go left!\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "  print(\"Step {}\".format(step + 1))\n",
    "  obs, reward, terminated, truncated, info = env.step(GO_LEFT)\n",
    "  print('obs=', obs, 'reward=', reward, 'done=', terminated)\n",
    "  env.render()\n",
    "  if terminated:\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora con un modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 23.4     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 717      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.344   |\n",
      "|    explained_variance | 0.439    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.0148   |\n",
      "|    value_loss         | 0.0101   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 13.8     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 790      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.136   |\n",
      "|    explained_variance | 0.562    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.00201  |\n",
      "|    value_loss         | 0.00241  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 9.68      |\n",
      "|    ep_rew_mean        | 1         |\n",
      "| time/                 |           |\n",
      "|    fps                | 823       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0792   |\n",
      "|    explained_variance | -5.29     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -0.000377 |\n",
      "|    value_loss         | 0.00144   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 9.28      |\n",
      "|    ep_rew_mean        | 1         |\n",
      "| time/                 |           |\n",
      "|    fps                | 850       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0585   |\n",
      "|    explained_variance | 0.281     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -5.01e-05 |\n",
      "|    value_loss         | 0.000447  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 9.12      |\n",
      "|    ep_rew_mean        | 1         |\n",
      "| time/                 |           |\n",
      "|    fps                | 858       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0319   |\n",
      "|    explained_variance | -0.653    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -0.000144 |\n",
      "|    value_loss         | 0.00125   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.1      |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 832      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00588 |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 7.86e-06 |\n",
      "|    value_loss         | 0.000122 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 9.08      |\n",
      "|    ep_rew_mean        | 1         |\n",
      "| time/                 |           |\n",
      "|    fps                | 771       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00945  |\n",
      "|    explained_variance | 0.539     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -2.98e-05 |\n",
      "|    value_loss         | 0.000637  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.02     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 762      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00366 |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 3.38e-06 |\n",
      "|    value_loss         | 7.85e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 9.02     |\n",
      "|    ep_rew_mean        | 1        |\n",
      "| time/                 |          |\n",
      "|    fps                | 761      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00534 |\n",
      "|    explained_variance | 0.74     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 1.09e-05 |\n",
      "|    value_loss         | 0.000503 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 9.02      |\n",
      "|    ep_rew_mean        | 1         |\n",
      "| time/                 |           |\n",
      "|    fps                | 761       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00181  |\n",
      "|    explained_variance | 0.968     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -5.12e-07 |\n",
      "|    value_loss         | 1.23e-05  |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = GoLeftEnv(grid_size=10)\n",
    "# wrap it\n",
    "env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "model = sb.A2C('MlpPolicy', env, verbose=1).learn(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RK4(fun, x, dt, t = 0, a = 0):\n",
    "\n",
    "    k1 = fun(t,      x,         a)\n",
    "    k2 = fun(t+dt/2, x+dt*k1/2, a)\n",
    "    k3 = fun(t+dt/2, x+dt*k2/2, a)\n",
    "    k4 = fun(t+dt,   x+dt*k3,   a)\n",
    "\n",
    "    y = x + dt/6*(k1+2*k2+2*k3+k4)\n",
    "    return y\n",
    "\n",
    "# Todo esto deberia ir dentro del Env eventualmente\n",
    "\n",
    "gamma     = .1\n",
    "gammath   = .1\n",
    "L     = .2\n",
    "G     = .98\n",
    "\n",
    "def cart_evol(t, x, a = 0):\n",
    "    vDot = a - gamma * x[1]\n",
    "    xDot = x[1]\n",
    "    return np.array([xDot, vDot])\n",
    "\n",
    "def pend_evol(t, x, a = 0):\n",
    "    thDotDot = (a * np.cos(x[0]) - G * np.sin(x[0]))/L - gammath * x[1]\n",
    "    thDot    = x[1]\n",
    "    return np.array([thDot, thDotDot])\n",
    "\n",
    "def get_pos_pend(ang, x, L):\n",
    "    return x - L*np.sin(ang), L*np.cos(ang)\n",
    "\n",
    "def observations(x,th,xDot,thDot):\n",
    "    return np.array([x, np.cos(th), np.sin(th), xDot/2, thDot/20]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "\n",
    "class Pendulo(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment that follows gym interface.\n",
    "    This is a simple env where the agent must learn to go always left. \n",
    "    \"\"\"\n",
    "    # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "    metadata = {'render.modes': ['console']}\n",
    "    # Define constants for clearer code\n",
    "    # Distancias en mm?\n",
    "\n",
    "    def __init__(self, fps = 120, target_th = 1, max_steps = 2000):\n",
    "        super(Pendulo, self).__init__()\n",
    "\n",
    "        # Size of the 1D-grid\n",
    "        self.rail_lengh = 1\n",
    "\n",
    "        # (x,th), (xDot,thDot)\n",
    "        self.agent_vars = np.array(((0.,0.),(0.,0.)))\n",
    "\n",
    "        # variables de tiempo\n",
    "        self.n_step = 0\n",
    "        self.max_steps = max_steps\n",
    "        self.dt      = 1000/fps # timestep en microseg\n",
    "\n",
    "        # threshold del reward\n",
    "        self.targetH = np.cos(target_th)\n",
    "\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions, we have 3: left, right and still\n",
    "        self.n_actions = 3\n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "        # The observation will be the coordinate of the agent\n",
    "        # (x, v, cos th, sin th, thDot)\n",
    "        # elijo pasarle seno y coseno enves del angulo xq quedan en [-1,1]\n",
    "        # y no tienen el problema de discontinuidad de th en [-pi,pi]\n",
    "        self.observation_space = spaces.Box(low = -1, high = 1,\n",
    "                                            shape=(5,), dtype=np.float32)\n",
    "\n",
    "    def reset(self, seed = None,):\n",
    "        \"\"\"\n",
    "        Important: the observation must be a numpy array\n",
    "        :return: (np.array) \n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        # Initialize the agent at the right of the grid\n",
    "        self.agent_vars = np.array(((0.,0.),(0.,0.)))\n",
    "        # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "        return observations(*np.ravel(self.agent_vars)), {}\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        x,v = self.agent_vars[0]\n",
    "        th,thDot = self.agent_vars[1]\n",
    "\n",
    "        if action > self.n_actions:\n",
    "            raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
    "        elif abs(x) > 1:\n",
    "            x = np.clip(x, -1, 1)\n",
    "            a = -v/self.dt * .2\n",
    "            thDotDot = -v/self.dt * 1.2\n",
    "            v = 0\n",
    "        else:\n",
    "            thDotDot = a = action - 1\n",
    "\n",
    "        x, v = RK4(cart_evol, [x, v], self.dt, a = a)\n",
    "        th, thDot = RK4(pend_evol, [th, thDot], self.dt, a = thDotDot)\n",
    "\n",
    "        self.agent_vars = np.array(((x,th),(v,thDot)))\n",
    "\n",
    "        terminated = False\n",
    "        if self.n_step >= 1500:\n",
    "            truncated = True\n",
    "        else: truncated = False\n",
    "        # Null reward everywhere except when reaching the goal (left of the grid)\n",
    "        reward = 1 if np.cos(th) < self.targetH else 0\n",
    "        # Optionally we can pass additional info, we are not using that for now\n",
    "        info = {}\n",
    "\n",
    "        return observations(*np.ravel(self.agent_vars)),\\\n",
    "                reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode='console'):\n",
    "        # if mode != 'console':\n",
    "        #     # agent is represented as a cross, rest as a dot\n",
    "        #     print(\".\" * self.agent_pos, end=\"\")\n",
    "        #     print(\"x\", end=\"\")\n",
    "        #     print(\".\" * (self.grid_size - self.agent_pos))\n",
    "        # else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The observation returned by the `step()` method does not match the bounds of the given observation space Box(-1.0, 1.0, (5,), float32). \n3 invalid indices: \nExpected: -1.0 <= obs[0] <= 1.0, actual value: 27.0865478515625 \nExpected: -1.0 <= obs[3] <= 1.0, actual value: 2.8123393058776855 \nExpected: -1.0 <= obs[4] <= 1.0, actual value: 1.2056987285614014 \n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m Pendulo()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# If the environment don't follow the interface, an error will be thrown\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m check_env(env, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\Pendulum\\Lib\\site-packages\\stable_baselines3\\common\\env_checker.py:482\u001b[0m, in \u001b[0;36mcheck_env\u001b[1;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# ============ Check the returned values ===============\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m _check_returned_values(env, observation_space, action_space)\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# ==== Check the render method and the declared render modes ====\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_render_check:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\Pendulum\\Lib\\site-packages\\stable_baselines3\\common\\env_checker.py:346\u001b[0m, in \u001b[0;36m_check_returned_values\u001b[1;34m(env, observation_space, action_space)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking key=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     _check_obs(obs, observation_space, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    348\u001b[0m \u001b[38;5;66;03m# We also allow int because the reward will be cast to float\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(reward, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe reward returned by `step()` must be a float\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\Pendulum\\Lib\\site-packages\\stable_baselines3\\common\\env_checker.py:251\u001b[0m, in \u001b[0;36m_check_obs\u001b[1;34m(obs, observation_space, method_name)\u001b[0m\n\u001b[0;32m    245\u001b[0m                 index_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, index))\n\u001b[0;32m    246\u001b[0m                 message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    247\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlower_bounds[index]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m <= obs[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] <= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mupper_bounds[index]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobs[index]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m                 )\n\u001b[1;32m--> 251\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(message)\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m observation_space\u001b[38;5;241m.\u001b[39mcontains(obs), (\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe observation returned by the `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()` method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match the given observation space \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m )\n",
      "\u001b[1;31mAssertionError\u001b[0m: The observation returned by the `step()` method does not match the bounds of the given observation space Box(-1.0, 1.0, (5,), float32). \n3 invalid indices: \nExpected: -1.0 <= obs[0] <= 1.0, actual value: 27.0865478515625 \nExpected: -1.0 <= obs[3] <= 1.0, actual value: 2.8123393058776855 \nExpected: -1.0 <= obs[4] <= 1.0, actual value: 1.2056987285614014 \n"
     ]
    }
   ],
   "source": [
    "env = Pendulo()\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pendulum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
