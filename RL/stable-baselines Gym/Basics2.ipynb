{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "import stable_baselines3 as sb\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renders the model in the enviroment to see it learning progress\n",
    "def show_progress(model, time_steps = 1000, deterministic = True):\n",
    "    env = model.get_env()\n",
    "    obs = env.reset()\n",
    "    for i in range(time_steps):\n",
    "        action, _states = model.predict(obs, deterministic = deterministic)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        env.render(\"human\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardar el modelo permite tener redes preentrenadas para no arrancar de cero cada vez si se quiere probar algo\n",
    "\n",
    "AdemÃ¡s se pueden cambiar los parametros del modelo luego de cargarlo. En este caso se cambia gamma y verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create save dir (esta en el disco C)\n",
    "save_dir = \"/tmp/gym/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model = sb.A2C('MlpPolicy', 'Pendulum-v1', verbose=0, gamma=0.9, n_steps=20).learn(8000)\n",
    "# The model will be saved under A2C_tutorial.zip\n",
    "model.save(save_dir + \"/A2C_tutorial\")\n",
    "\n",
    "del model # delete trained model to demonstrate loading\n",
    "\n",
    "# load the model, and when loading set verbose to 1\n",
    "loaded_model = sb.A2C.load(save_dir + \"/A2C_tutorial\", verbose = 1, gamma = .8)\n",
    "\n",
    "# show the save hyperparameters\n",
    "print(\"loaded:\", \"gamma =\", loaded_model.gamma, \"n_steps =\", loaded_model.n_steps)\n",
    "\n",
    "# as the environment is not serializable, we need to set a new instance of the environment\n",
    "loaded_model.set_env(sb.common.vec_env.DummyVecEnv([lambda: gym.make('Pendulum-v1')]))\n",
    "# and continue training\n",
    "loaded_model.learn(8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A gym wrapper follows the gym interface: it has a reset() and step() method.\n",
    "\n",
    "We can access it with self.env allowing to easily interact with it without modifying the original env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anatomia Basica\n",
    "# class CustomWrapper(gym.Wrapper):\n",
    "#   \"\"\"\n",
    "#   :param env: (gym.Env) Gym environment that will be wrapped\n",
    "#   \"\"\"\n",
    "#   def __init__(self, env):\n",
    "#     # Call the parent constructor, so we can access self.env later\n",
    "#     super(CustomWrapper, self).__init__(env)\n",
    "  \n",
    "#   def reset(self):\n",
    "#     \"\"\"\n",
    "#     Reset the environment \n",
    "#     \"\"\"\n",
    "#     obs = self.env.reset()\n",
    "#     return obs\n",
    "\n",
    "#   def step(self, action):\n",
    "#     \"\"\"\n",
    "#     :param action: ([float] or int) Action taken by the agent\n",
    "#     :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
    "#     \"\"\"\n",
    "#     obs, reward, done, info = self.env.step(action)\n",
    "#     return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding/Changing the TimeLimit per Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hagamos modificaciones:\n",
    "#   Limitamos los time_steps por episodio\n",
    "class CustomWrapper(gym.Wrapper):\n",
    "  \"\"\"\n",
    "  :param env: (gym.Env) Gym environment that will be wrapped\n",
    "  :param max_steps: (int) Max number of steps per episode\n",
    "  \"\"\"\n",
    "  def __init__(self, env, max_steps = 100):\n",
    "    # Call the parent constructor, so we can access self.env later\n",
    "    super(CustomWrapper, self).__init__(env)\n",
    "    self.max_steps = max_steps\n",
    "    # Counter of steps per episode\n",
    "    self.current_step = 0\n",
    "  \n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Reset the environment \n",
    "    \"\"\"\n",
    "    # Reset the counter\n",
    "    self.current_step = 0\n",
    "    return self.env.reset()\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"\n",
    "    :param action: ([float] or int) Action taken by the agent\n",
    "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
    "    \"\"\"\n",
    "    self.current_step += 1\n",
    "    obs, reward, done, truncated, info = self.env.step(action)\n",
    "    # Overwrite the done signal when \n",
    "    if self.current_step >= self.max_steps:\n",
    "      truncated = True\n",
    "      # Update the info dict to signal that the limit was exceeded\n",
    "      info['time_limit_reached'] = True\n",
    "    return obs, reward, done, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 {'time_limit_reached': True}\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.envs.classic_control.pendulum import PendulumEnv\n",
    "\n",
    "# gym.make() already wrap the environement in a TimeLimit wrapper\n",
    "env = PendulumEnv()\n",
    "env = CustomWrapper(env, max_steps=100)\n",
    "\n",
    "obs = env.reset()\n",
    "done, truncated = False, False\n",
    "n_steps = 0\n",
    "while not (done or truncated):\n",
    "  # Take random actions\n",
    "  random_action = env.action_space.sample()\n",
    "  obs, reward, done, truncated, info = env.step(random_action)\n",
    "  n_steps += 1\n",
    "\n",
    "print(n_steps, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reescaling Actions\n",
    "\n",
    "It's a good idea to work with normalize observations and actions -in a [-1,1] range-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWrapper(gym.Wrapper):\n",
    "  \"\"\"\n",
    "  :param env: (gym.Env) Gym environment that will be wrapped\n",
    "  \"\"\"\n",
    "  def __init__(self, env):\n",
    "    # Retrieve the action space\n",
    "    action_space = env.action_space\n",
    "    assert isinstance(action_space, gym.spaces.Box), \"This wrapper only works with continuous action space (spaces.Box)\"\n",
    "    # Retrieve the max/min values\n",
    "    self.low, self.high = action_space.low, action_space.high\n",
    "\n",
    "    # We modify the action space, so all actions will lie in [-1, 1]\n",
    "    env.action_space = gym.spaces.Box(low=-1, high=1, shape=action_space.shape, dtype=np.float32)\n",
    "\n",
    "    # Call the parent constructor, so we can access self.env later\n",
    "    super(CustomWrapper, self).__init__(env)\n",
    "  \n",
    "  def rescale_action(self, scaled_action):\n",
    "      \"\"\"\n",
    "      Rescale the action from [-1, 1] to [low, high]\n",
    "      (no need for symmetric action space)\n",
    "      :param scaled_action: (np.ndarray)\n",
    "      :return: (np.ndarray)\n",
    "      \"\"\"\n",
    "      return self.low + (0.5 * (scaled_action + 1.0) * (self.high -  self.low))\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Reset the environment \n",
    "    \"\"\"\n",
    "    # Reset the counter\n",
    "    return self.env.reset()\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"\n",
    "    :param action: ([float] or int) Action taken by the agent\n",
    "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
    "    \"\"\"\n",
    "    # Rescale action from [-1, 1] to original [low, high] interval\n",
    "    rescaled_action = self.rescale_action(action)\n",
    "    obs, reward, done, info = self.env.step(rescaled_action)\n",
    "    return obs, reward, done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original action Range:  [-2.] [2.]\n",
      "Rescaled action Range:  [-1.] [1.]\n",
      "[1.9145433] \t [-0.82647973]\n",
      "[-1.8798988] \t [-0.15079382]\n",
      "[0.06876159] \t [0.795015]\n",
      "[-0.10251061] \t [0.32899192]\n",
      "[1.640153] \t [0.38537577]\n",
      "[0.2384393] \t [0.66193366]\n",
      "[-1.522139] \t [0.24981137]\n",
      "[1.8707141] \t [0.26488578]\n",
      "[-0.8182553] \t [-0.724749]\n",
      "[1.6064415] \t [-0.57995605]\n",
      "Original action Range:  [-2.] [2.]\n",
      "Rescaled action Range:  [-1.] [1.]\n"
     ]
    }
   ],
   "source": [
    "original_env = gym.make(\"Pendulum-v1\")\n",
    "env = CustomWrapper(gym.make(\"Pendulum-v1\"))\n",
    "\n",
    "print(\"Original action Range: \", original_env.action_space.low, original_env.action_space.high)\n",
    "print(\"Rescaled action Range: \", env.action_space.low, env.action_space.high)\n",
    "for _ in range(10):\n",
    "  print(original_env.action_space.sample(),\"\\t\",env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vec Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pendulum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
