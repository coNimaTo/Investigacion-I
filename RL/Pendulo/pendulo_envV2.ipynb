{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import stable_baselines3 as sb\n",
    "\n",
    "import pygame\n",
    "import pygame.font\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "# from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLACK = (0  ,0  ,0  )\n",
    "WHITE = (255,255,255)\n",
    "GREEN = (0  ,150,40 )\n",
    "\n",
    "WIDTH, HEIGHT = 500, 300\n",
    "CART_SIZE = (40,20)\n",
    "CART_POS  = (WIDTH/2-20, HEIGHT/2-10) # (screen- cart)/2\n",
    "\n",
    "gamma     = .1\n",
    "gammath   = .1\n",
    "L     = .2\n",
    "G     = .98\n",
    "\n",
    "def RK4(fun, x, dt, t = 0, a = 0):\n",
    "\n",
    "    k1 = fun(t,      x,         a)\n",
    "    k2 = fun(t+dt/2, x+dt*k1/2, a)\n",
    "    k3 = fun(t+dt/2, x+dt*k2/2, a)\n",
    "    k4 = fun(t+dt,   x+dt*k3,   a)\n",
    "\n",
    "    y = x + dt/6*(k1+2*k2+2*k3+k4)\n",
    "    return y\n",
    "\n",
    "# Todo esto deberia ir dentro del Env eventualmente\n",
    "\n",
    "def cart_evol(t, x, a = 0):\n",
    "    vDot = a - gamma * x[1]\n",
    "    xDot = x[1]\n",
    "    return np.array([xDot, vDot])\n",
    "\n",
    "def pend_evol(t, x, a = 0):\n",
    "    thDotDot = (a * np.cos(x[0]) - G * np.sin(x[0]))/L - gammath * x[1]\n",
    "    thDot    = x[1]\n",
    "    return np.array([thDot, thDotDot])\n",
    "\n",
    "def get_pos_pend(ang, x):\n",
    "    l = L*WIDTH\n",
    "    return x[0] - l*np.sin(ang), x[1] + l*np.cos(ang)\n",
    "\n",
    "def draw_player(screen, cart, pend_pos):\n",
    "    pygame.draw.line(screen, BLACK,\n",
    "                    cart.center, pend_pos,\n",
    "                    width = 6)\n",
    "    pygame.draw.rect(screen, WHITE, cart)\n",
    "    pygame.draw.rect(screen, BLACK, cart, 4)\n",
    "    pygame.draw.circle(screen, WHITE, pend_pos, 20)\n",
    "    pygame.draw.circle(screen, BLACK, pend_pos, 20, 5)\n",
    "\n",
    "def update_screen(screen, cart, th):\n",
    "    screen.fill((255,255,255))\n",
    "    pend_pos  = get_pos_pend(th, cart.center)\n",
    "\n",
    "    # pygame.draw.line(screen, BLACK, (1000, HEIGHT*(1-.1)/2), (100, HEIGHT*(1+.1)/2), 4)\n",
    "    pygame.draw.line(screen, BLACK, (0, HEIGHT/2), (WIDTH, HEIGHT/2), 4)\n",
    "    # pygame.draw.line(screen, BLACK, (1440, HEIGHT*(1-.1)/2), (1440, HEIGHT*(1+.1)/2), 4)\n",
    "    draw_player(screen, cart, pend_pos)\n",
    "    \n",
    "def observations(x,th,xDot,thDot):\n",
    "    return np.array([x, np.cos(th), np.sin(th), xDot/2, thDot/20]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pendulo(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment that follows gym interface.\n",
    "    This is a simple env where the agent must learn to go always left. \n",
    "    \"\"\"\n",
    "    # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "    metadata = {'render.modes': ['data', \"pygame\"]}\n",
    "    # Define constants for clearer code\n",
    "    # Distancias en mm?\n",
    "\n",
    "    def __init__(self, fps = 120, render_mode = None):\n",
    "        super(Pendulo, self).__init__()   \n",
    "\n",
    "        # (x,th), (xDot,thDot)\n",
    "        self.agent_vars = np.array(((0.,np.random.normal(0,.1)),(0.,0.)))\n",
    "\n",
    "        # variables de tiempo\n",
    "        self.dt = 1/fps # timestep en seg\n",
    "\n",
    "        # threshold del reward\n",
    "        #self.targetH = np.cos(target_th)\n",
    "        self.total_reward = 0\n",
    "\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        self.action_space = spaces.Box(low = -1, high = 1,\n",
    "                                            shape=(1,), dtype=np.float32)\n",
    "        # The observation will be the coordinate of the agent\n",
    "        # (x, v, cos th, sin th, thDot)\n",
    "        # elijo pasarle seno y coseno enves del angulo xq quedan en [-1,1]\n",
    "        # y no tienen el problema de discontinuidad de th en [-pi,pi]\n",
    "        self.observation_space = spaces.Box(low = -1, high = 1,\n",
    "                                            shape=(5,), dtype=np.float32)\n",
    "        \n",
    "        self.render_mode = render_mode\n",
    "        if render_mode == \"data\":\n",
    "            with open(\"output.txt\", \"w\") as f:\n",
    "                f.write(\"x\\tcos(th)\\tsin(th)\\tv\\tthDot\\t\\n\")\n",
    "        elif render_mode == \"pygame\":\n",
    "            pygame.init()\n",
    "            pygame.font.init()\n",
    "\n",
    "            self.screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "            # self.clock  = pygame.time.Clock()\n",
    "            self.player = pygame.Rect(CART_POS + CART_SIZE)\n",
    "            self.my_font = pygame.font.SysFont('Comic Sans MS', 50) \n",
    "\n",
    "    def reset(self, seed = 0,):\n",
    "        \"\"\"\n",
    "        Important: the observation must be a numpy array\n",
    "        :return: (np.array) \n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Initialize the agent at the right of the grid\n",
    "        self.agent_vars = np.array(((0.,np.random.normal(0,.1)),(0.,0.)))\n",
    "        self.total_reward = 0\n",
    "        if self.render_mode == \"pygame\":\n",
    "            pygame.display.update()\n",
    "            self.player.move_ip(WIDTH/2 - self.player.x, 0)\n",
    "            update_screen(self.screen, self.player, self.agent_vars[0][1])\n",
    "        \n",
    "        return observations(*np.ravel(self.agent_vars)), {}\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        x,th = self.agent_vars[0]\n",
    "        v,thDot = self.agent_vars[1]\n",
    "\n",
    "        if abs(action[0]) > 1:\n",
    "            raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
    "        elif abs(x) > 1:\n",
    "            # x = np.clip(x, -1, 1)\n",
    "            # a = -v/self.dt * .2\n",
    "            # thDotDot = -v/self.dt * 1.2\n",
    "            # v = 0\n",
    "            return observations(*np.ravel(self.agent_vars)),\\\n",
    "                -10000, True, False, {\"Crashed\" : True}\n",
    "        else:\n",
    "            thDotDot = a = action[0] * 3\n",
    "\n",
    "        x, v = RK4(cart_evol, [x, v], self.dt, a = a)\n",
    "        th, thDot = RK4(pend_evol, [th, thDot], self.dt, a = thDotDot)\n",
    "\n",
    "        # print(x,v,th,thDot)\n",
    "\n",
    "        self.agent_vars = np.array(((x,th),(v,thDot)))\n",
    "        \n",
    "        # reward\n",
    "        if np.cos(th) < 0:\n",
    "            reward = 1/(1 + 10*abs(x) + abs(thDot))\n",
    "        else:\n",
    "            reward = -np.cos(th)*(4+abs(x))\n",
    "        self.total_reward += reward\n",
    "        # Optionally we can pass additional info, we are not using that for now\n",
    "        info = {}\n",
    "\n",
    "        return observations(*np.ravel(self.agent_vars)),\\\n",
    "                reward, False, False, info\n",
    "\n",
    "    def render(self, seed = 0):\n",
    "        if self.render_mode == 'data':\n",
    "            # agent is represented as a cross, rest as a dot\n",
    "            data = observations(*np.ravel(self.agent_vars))\n",
    "            with open(\"output.txt\", \"a\") as f:\n",
    "                for var in data:\n",
    "                    f.write(str(var))\n",
    "                    f.write(\"\\t\")\n",
    "                f.write(\"\\n\")\n",
    "        elif self.render_mode == \"pygame\":\n",
    "            x,th = self.agent_vars[0]\n",
    "            x = WIDTH/2 * (x+1)\n",
    "            self.player.move_ip(x-self.player.x, 0)\n",
    "\n",
    "            update_screen(self.screen, self.player, th)\n",
    "            self.screen.blit(self.my_font.render(\"SCORE: %.1f\"%(self.total_reward), False, GREEN), (100,50))\n",
    "            pygame.display.update()\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Pendulo()\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)\n",
    "# gym.register(\n",
    "#     id=\"Pendulo\",\n",
    "#     entry_point=Pendulo,\n",
    "# )\n",
    "# wrap it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_env = make_vec_env(Pendulo , n_envs = 16)\n",
    "model = sb.PPO('MlpPolicy', multi_env,\n",
    "               n_steps = 1024, batch_size = 64)\n",
    "# model = sb.PPO.load(save_dir + \"/PendulumV2_PPO_2E6Iterations_2envs\", multi_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1e92d024f50>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps = int(6e6), reset_num_timesteps = 1500)\n",
    "            #,progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 {}\n"
     ]
    }
   ],
   "source": [
    "single_env = Pendulo(render_mode = \"pygame\")\n",
    "obs, _ = single_env.reset()\n",
    "        \n",
    "for step in range(3000):\n",
    "    time.sleep(0.005)\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = single_env.step(action)\n",
    "    single_env.render()\n",
    "    #print(\"Step {}\".format(step + 1), \"\\t\", \"Action: \", action, \"\\t\", \"reward=\", reward)\n",
    "    if terminated or truncated:\n",
    "        # Note that the VecEnv resets automatically\n",
    "        # when a done signal is encountered\n",
    "        print(step+1, info)\n",
    "        break\n",
    "print(step+1, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hagamos modificaciones:\n",
    "#   Limitamos los time_steps por episodio\n",
    "class CustomWrapper(gym.Wrapper):\n",
    "  \"\"\"\n",
    "  :param env: (gym.Env) Gym environment that will be wrapped\n",
    "  :param max_steps: (int) Max number of steps per episode\n",
    "  \"\"\"\n",
    "  def __init__(self, env, max_steps = 100):\n",
    "    # Call the parent constructor, so we can access self.env later\n",
    "    super(CustomWrapper, self).__init__(env)\n",
    "    self.max_steps = max_steps\n",
    "    # Counter of steps per episode\n",
    "    self.current_step = 0\n",
    "  \n",
    "  def reset(self, seed = None):\n",
    "    \"\"\"\n",
    "    Reset the environment \n",
    "    \"\"\"\n",
    "    # Reset the counter\n",
    "    self.current_step = 0\n",
    "    return self.env.reset()\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"\n",
    "    :param action: ([float] or int) Action taken by the agent\n",
    "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
    "    \"\"\"\n",
    "    self.current_step += 1\n",
    "    obs, reward, done, truncated, info = self.env.step(action)\n",
    "    # Overwrite the done signal when \n",
    "    if self.current_step >= self.max_steps:\n",
    "      truncated = True\n",
    "      # Update the info dict to signal that the limit was exceeded\n",
    "      info['time_limit_reached'] = True\n",
    "    return obs, reward, done, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-225.53949138000004 959.1036871679634\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "eval_env = Monitor(CustomWrapper(Pendulo(), 2400))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100, deterministic=True)\n",
    "\n",
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/tmp/gym/\"\n",
    "model.save(save_dir + \"/PendulumV2_PPO_2E6Iterations_2envs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table(\"output.txt\")\n",
    "x, costh, sinth, v, thDot = data[\"x\"],data[\"cos(th)\"],data[\"sin(th)\"],data[\"v\"],data[\"thDot\"],\n",
    "\n",
    "fig, ax = plt.subplots(2,2)\n",
    "\n",
    "ax[0,0].plot(x)\n",
    "ax[0,1].plot(x - .2*sinth, - .2*costh)\n",
    "ax[1,0].plot(v)\n",
    "ax[1,1].plot(thDot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.collections as mcoll\n",
    "import matplotlib.path as mpath\n",
    "from cycler import cycler\n",
    "\n",
    "xx = x - .2*sinth\n",
    "yy = - .2*costh\n",
    "\n",
    "MAP = 'jet'\n",
    "NPOINTS = len(xx)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set(xlim = (-1,1), ylim = (-.25,.25))\n",
    "cm = plt.get_cmap(MAP)\n",
    "for i in range(10):\n",
    "    colors = [cm(1.0*i/(NPOINTS-1)) for i in range(NPOINTS-1)]\n",
    "    ax1.set_prop_cycle(cycler('color', colors))\n",
    "    for i in range(NPOINTS-1):\n",
    "        plt.plot(xx[i:i+2],yy[i:i+2])\n",
    "\n",
    "# plt.title('Inner minimization', fontsize=25)\n",
    "# plt.xlabel(r'Friction torque $[Nm]$', fontsize=25)\n",
    "# plt.ylabel(r'Accelerations energy $[\\frac{Nm}{s^2}]$', fontsize=25)\n",
    "plt.show() # Show the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6,3), dpi = 100)\n",
    "\n",
    "x1 = x\n",
    "zz = np.zeros(len(x))\n",
    "xx = x - .2*sinth\n",
    "yy = - .2*costh\n",
    "\n",
    "# x1 = np.array(x1[::10])\n",
    "# zz = np.array(zz[::10])\n",
    "# xx = np.array(xx[::10])\n",
    "# yy = np.array(yy[::10])\n",
    "\n",
    "rail = ax.plot([-1,1],[0,0], \"k\")[0]\n",
    "line = ax.plot([xx[0],x[0]],[yy[0],zz[0]], \"k:\")[0]\n",
    "\n",
    "scat = ax.scatter(xx[0], yy[0], c=\"b\", s=25)\n",
    "scat2 = ax.scatter(x[0], zz[0], c=\"k\", s=25)\n",
    "\n",
    "ax.set(xlim=[-1, 1], ylim=[-.5, .5], xticks=[], yticks=[])\n",
    "\n",
    "\n",
    "def update(frame):\n",
    "    # for each frame, update the data stored on each artist.\n",
    "    x = x1[frame]\n",
    "    z = zz[frame]\n",
    "    X = xx[frame]\n",
    "    Y = yy[frame]\n",
    "    # update the scatter plot:\n",
    "    data = np.stack([X, Y]).T\n",
    "    data2 = np.stack([x, z]).T\n",
    "    scat.set_offsets(data)\n",
    "    scat2.set_offsets(data2)\n",
    "    # update the line plot:\n",
    "    line.set_xdata([x,X])\n",
    "    line.set_ydata([z,Y])\n",
    "    return (line, scat)\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(fig=fig, func=update, frames = len(x1))\n",
    "ani.save(\"mov.mp4\", fps = 120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Inv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
